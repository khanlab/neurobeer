#!/usr/bin/env python
""" clusterUFiberPrior

Python command line interface for automated tractography clustering
and evaluation from previously clustered data

"""
def get_parser():
    """
    Argument Parser
    """
    from argparse import ArgumentParser, RawTextHelpFormatter
    from neurobeer._version import __version__

    parser = ArgumentParser(description=('U-fiber extraction and clustering '
                                         'based on prior clusters'),
                            formatter_class=RawTextHelpFormatter)

    # Version option
    parser.add_argument('--version', action='version', version=__version__)

    # Required arguments
    g_req = parser.add_argument_group('required arguments')
    g_req.add_argument('--indir', action='store', required=True,
                       help='the directory with input data')
    g_req.add_argument('--outdir', action='store', required=True,
                       help='the directory where output files should be stored')
    g_req.add_argument('--subjid', action='store', required=True,
                       help='subject id to compute')
    g_req.add_argument('--bundle', action='store', required=True,
                       help='tractography bundle to perform clustering on')
    g_req.add_argument('--prior', action='store', required=True,
                       help='directory where prior U-fiber data is stored')

    # Optional arguments
    g_opt = parser.add_argument_group('control arguments')
    g_opt.add_argument('-a', action='store', nargs='+', metavar='data',
                       default=[], help='add scalar data to be used')
    g_opt.add_argument('-w', action='store', nargs='+', metavar='wgt',
                       default=[], help=('provide weighting on data for '
                                         'clustering, '))
    g_opt.add_argument('-sig', action='store', nargs='+', type=float,
                       metavar='sigma', default=10,
                       help=('sigma to be used in clustering algorithm'))
    g_opt.add_argument('-j', action='store', type=int, metavar='n_jobs',
                       default=-1, help='number of cores to use')
    g_opt.add_argument('-t', action='store_true',
                       default=False, help='Flag for clustering template')
    g_opt.add_argument('-v', '--verbose', action='count', default=0,
                       help='verbosity of tool')

    return parser

def main():
    """
    Entry point of code
    """
    import os
    import numpy as np
    from neurobeer.tractography import (cluster, fibers, prior, stats, tractio,
                                       ufiber)

    # Run parser
    opts = get_parser().parse_args()

    # Read input polydata
    indir = os.path.realpath(os.path.join(opts.indir + '/' + opts.subjid))
    outdir = os.path.realpath(os.path.join(opts.outdir + '/' + opts.subjid))

    if not os.path.exists(outdir):
        os.makedirs(outdir)

    bundleVTK = os.path.join(indir + '/' + opts.bundle)
    bundlePolydata = tractio.readVTK(bundleVTK, opts.verbose)

    _, _, pts_per_fiber = prior.getFiberInfo(opts.prior)
    fiberData = fibers.FiberTree()
    fiberData.convertFromVTK(bundlePolydata, pts_per_fiber, opts.verbose)
    del bundleVTK

    # Handling scalar data
    scalarDataList, scalarWeightList, scalarTypeList = [], [], []
    if opts.a is not None:
        scalarFileList = opts.a
        for DataIdx in scalarFileList:
            path = os.path.join(indir + '/' + str(DataIdx))
            name = path.split('.', -1)[-2]
            name = name.split('/', -1)[-1]
            if name.split('-', -1)[0] == 'sub':
                name = name.split('_', -1)[-1]
            data = name + 'Data'
            typevar = name + 'Type'
            exec('%s, %s = tractio.readScalar("%s", opts.verbose)'
                % (data, typevar, path))
            exec('scalarDataList.append(%s)' % (data))
            exec('scalarTypeList.append(%s)' % (typevar))

    if opts.w is not None:
        for val in opts.w:
            scalarWeightList.append(float(val))

    for i in range(len(scalarTypeList)):
        fiberData.addScalar(bundlePolydata, scalarDataList[i],
                            scalarTypeList[i], fiberData.pts_per_fiber)

    # Extract u-fibers
    uArray, L, D = ufiber.findUFiber(fiberData)
    uFiberTree = ufiber.extractUFiber(fiberData, uArray)
    uFiberTree.copyScalar(fiberData, scalarTypeList, fidxes=uArray)

    # Perform clustering on provided bundle
    tractdir = os.path.join(outdir, 'tractography')
    if not os.path.exists(tractdir):
        os.makedirs(tractdir)

    outputPolydata, clusterIdx, fiberData, rejIdx = \
                                    cluster.spectralPriorCluster(
                                        uFiberTree, opts.prior,
                                        opts.t, scalarDataList=scalarDataList,
                                        scalarWeightList=scalarWeightList,
                                        scalarTypeList=scalarTypeList,
                                        sigma=opts.sig, dirpath=tractdir,
                                        n_jobs=opts.j, verbose=opts.verbose)
    del bundlePolydata, opts.prior, scalarWeightList, scalarDataList

    bundleName = opts.bundle[:-4] + '_uFibers_Clustered.vtk'
    bundleName = bundleName.split('/', -1)[-1]
    bundledir = os.path.join(tractdir, bundleName)
    tractio.writeVTK(outputPolydata, bundledir, opts.verbose)
    clusterData = fiberData.getFibers(range(fiberData.no_of_fibers), rejIdx)
    clusterData = fibers.convertFromTuple(clusterData)
    clusterData.copyScalar(fiberData, scalarTypeList, fidxes=[], rejIdx=rejIdx)

    # Extract individual clusters
    statsdir = os.path.join(tractdir, 'stats')
    if not os.path.exists(statsdir):
        os.makedirs(statsdir)

    for label in np.unique(clusterIdx):
        idxes = np.where(clusterIdx == label)[0]
        bundle = clusterData.getFibers(idxes)
        bundle = fibers.convertFromTuple(bundle)
        polyData = bundle.convertToVTK()
        statsSuffix = 'stats_%i' % label
        labeldir = os.path.join(statsdir, statsSuffix)
        LMean, LStd, DMean, DStd = ufiber.uFiberStats(L, D, idxes)
        ufiber.writeCSV(label, LMean, LStd, DMean, DStd, bundle.no_of_fibers,
                        dirpath=statsdir)
        print("\nAvg. fiber length for cluster %i: %.2f +/- %.2f"
                % (label, LMean, LStd))
        print("Avg. distance between end points for cluster %i: %.2f +/- %.2f"
                % (label, DMean, DStd))

        for Type in scalarTypeList:
            polyData = cluster.addScalarToVTK(polyData, clusterData, Type,
                                              idxes)
            stats.plotStats(clusterData, Type, idxes, dirpath=labeldir)
            stats.writeCSV(label, clusterData, Type, idxes, dirpath=statsdir)

        bundleSuffix = '_uFibers_Cluster%i.vtk' % label
        bundleName = opts.bundle[:-4] + bundleSuffix
        bundleName = bundleName.split('/', -1)[-1]
        bundledir = os.path.join(tractdir, bundleName)
        tractio.writeVTK(polyData, bundledir, opts.verbose)


if __name__ == '__main__':
    main()
